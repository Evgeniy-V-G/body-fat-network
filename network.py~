import csv
import numpy as np
import tensorflow as tf
import math

class BodyNetwork():

	###############
	def __init__(self,):
		print('init')
		
	
	def dense_network(self, inputs,input_size, hidden1_units, hidden2_units):

		# Hidden 1
		with tf.name_scope('hidden1'):
			# Random values to initialize
			weights = tf.Variable(tf.truncated_normal([input_size, hidden1_units],
						          stddev=1.0 / math.sqrt(float(input_size))),
			   					  name='weights')
			# Constant inputs 
			biases = tf.Variable(tf.zeros([hidden1_units]),name='biases')

			# Activation Function
			hidden1 = tf.nn.relu(tf.matmul(inputs, weights) + biases)
		
		# Hidden 2
		with tf.name_scope('hidden2'):
			weights = tf.Variable(tf.truncated_normal([hidden1_units,hidden2_units],
						          stddev=1.0 / math.sqrt(float(hidden1_units))),
								  name='weights')
							
			biases = tf.Variable(tf.zeros([hidden2_units]),name='biases')
			hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)
		
		# Linear
		with tf.name_scope('output'):
			weights = tf.Variable(tf.truncated_normal([hidden2_units, 1],
						          stddev=1.0 / math.sqrt(float(hidden2_units))),
						          name='weights')

			biases = tf.Variable(tf.zeros([1]),name='biases')

			output = tf.matmul(hidden2, weights) + biases
			output2 = tf.reshape(output, [-1])

		return output2

	def loss(self, ground_truth, predictions):

		# Fidelity term
		loss = tf.losses.log_loss(ground_truth, predictions)
		loss = tf.Variable(10,dtype=tf.float32)
		#loss = tf.reduce_mean(ground_truth-predictions)

		# Regularisation term
		#regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)
		#reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
		#reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)

		# Total Loss
		#loss += reg_term
		return loss
	  
	def training(self,loss, learning_rate):

		# Add a scalar summary for the snapshot loss.
		#tf.scalar_summary(loss.op.name, loss)

		# Create the gradient descent optimizer with the given learning rate.
		optimizer = tf.train.GradientDescentOptimizer(learning_rate)

		# Create a variable to track the global step.
		global_step = tf.Variable(0, name='global_step', trainable=False)
		# Use the optimizer to apply the gradients that minimize the loss
		# (and also increment the global step counter) as a single training step.
		train_op = optimizer.minimize(loss, global_step=global_step)
		return train_op
